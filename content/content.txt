









Build a Simple LLM Application with LCEL | ü¶úÔ∏èüîó LangChain
















Skip to main content
Share your thoughts on AI agents. 
Take the 3-min survey
.
Integrations
API reference
Latest
Legacy
More
People
Contributing
Cookbooks
3rd party tutorials
YouTube
arXiv
v0.3
v0.3
v0.2
v0.1
ü¶úÔ∏èüîó
LangSmith
LangSmith Docs
LangChain Hub
JS/TS Docs
üí¨
Search
Introduction
Tutorials
Build a Question Answering application over a Graph Database
Tutorials
Build a Simple LLM Application with LCEL
Build a Query Analysis System
Build a Chatbot
Conversational RAG
Build an Extraction Chain
Build an Agent
Tagging
data_generation
Build a Local RAG Application
Build a PDF ingestion and Question/Answering system
Build a Retrieval Augmented Generation (RAG) App
Vector stores and retrievers
Build a Question/Answering system over SQL data
Summarize Text
How-to guides
How-to guides
How to use tools in a chain
How to use a vectorstore as a retriever
How to add memory to chatbots
How to use example selectors
How to map values to a graph database
How to add a semantic layer over graph database
How to invoke runnables in parallel
How to stream chat model responses
How to add default invocation args to a Runnable
How to add retrieval to chatbots
How to use few shot examples in chat models
How to do tool/function calling
How to best prompt for Graph-RAG
How to install LangChain packages
How to add examples to the prompt for query analysis
How to use few shot examples
How to run custom functions
How to use output parsers to parse an LLM response into structured format
How to handle cases where no queries are generated
How to route between sub-chains
How to return structured data from a model
How to summarize text through parallelization
How to summarize text through iterative refinement
How to summarize text in a single LLM call
How to use toolkits
How to add ad-hoc tool calling capability to LLMs and Chat Models
Build an Agent with AgentExecutor (Legacy)
How to construct knowledge graphs
How to partially format prompt templates
How to handle multiple queries when doing query analysis
How to use built-in tools and toolkits
How to pass through arguments from one step to the next
How to compose prompts together
How to handle multiple retrievers when doing query analysis
How to add values to a chain's state
How to construct filters for query analysis
How to configure runtime chain internals
How deal with high cardinality categoricals when doing query analysis
Custom Document Loader
How to split by HTML header
How to split by HTML sections
How to use the MultiQueryRetriever
How to add scores to retriever results
Caching
How to use callbacks in async environments
How to attach callbacks to a runnable
How to propagate callbacks  constructor
How to dispatch custom callback events
How to pass callbacks in at runtime
How to split by character
How to cache chat model responses
How to handle rate limits
How to init any model in one line
How to track token usage in ChatModels
How to add tools to chatbots
How to split code
How to do retrieval with contextual compression
How to convert Runnables as Tools
How to create custom callback handlers
How to create a custom chat model class
How to create a custom LLM class
Custom Retriever
How to create tools
How to debug your LLM apps
How to load CSVs
How to load documents from a directory
How to load HTML
How to load JSON
How to load Markdown
How to load Microsoft Office files
How to load PDFs
How to create a dynamic (self-constructing) chain
Text embedding models
How to combine results from multiple retrievers
How to select examples from a LangSmith dataset
How to select examples by length
How to select examples by maximal marginal relevance (MMR)
How to select examples by n-gram overlap
How to select examples by similarity
How to use reference examples when doing extraction
How to handle long text when doing extraction
How to use prompting alone (no tool calling) to do extraction
How to add fallbacks to a runnable
How to filter messages
Hybrid Search
How to use the LangChain indexing API
How to inspect runnables
LangChain Expression Language Cheatsheet
How to cache LLM responses
How to track token usage for LLMs
Run models locally
How to get log probabilities
How to reorder retrieved results to mitigate the "lost in the middle" effect
How to split Markdown by Headers
How to merge consecutive messages of the same type
How to add message history
How to migrate from legacy LangChain agents to LangGraph
How to retrieve using multiple vectors per document
How to pass multimodal data directly to models
How to use multimodal prompts
How to create a custom Output Parser
How to use the output-fixing parser
How to parse JSON output
How to retry when a parsing error occurs
How to parse XML output
How to parse YAML output
How to use the Parent Document Retriever
How to use LangChain with different Pydantic versions
How to add chat history
How to get a RAG application to add citations
How to do per-user retrieval
How to get your RAG application to return sources
How to stream results from your RAG application
How to split JSON data
How to recursively split text by characters
Response metadata
How to pass runtime secrets to runnables
How to do "self-querying" retrieval
How to split text based on semantic similarity
How to chain runnables
How to save and load LangChain objects
How to split text by tokens
How to do question answering over CSVs
How to deal with large databases when doing SQL question-answering
How to better prompt when doing SQL question-answering
How to do query validation as part of SQL question-answering
How to stream runnables
How to stream responses from an LLM
How to use a time-weighted vector store retriever
How to return artifacts from a tool
How to use chat models to call tools
How to disable parallel tool calling
How to force models to call a tool
How to access the RunnableConfig from a tool
How to pass tool outputs to chat models
How to pass run time values to tools
How to stream events from a tool
How to stream tool calls
How to convert tools to OpenAI Functions
How to handle tool errors
How to use few-shot prompting with tool calling
How to add a human-in-the-loop for tools
How to bind model-specific tools
How to trim messages
How to create and query vector stores
Conceptual guide
Ecosystem
ü¶úüõ†Ô∏è LangSmith
ü¶úüï∏Ô∏è LangGraph
Versions
v0.3
v0.2
Pydantic compatibility
Migrating from v0.0 chains
How to migrate from v0.0 chains
Migrating from ConstitutionalChain
Migrating from ConversationalChain
Migrating from ConversationalRetrievalChain
Migrating from LLMChain
Migrating from LLMMathChain
Migrating from LLMRouterChain
Migrating from MapReduceDocumentsChain
Migrating from MapRerankDocumentsChain
Migrating from MultiPromptChain
Migrating from RefineDocumentsChain
Migrating from RetrievalQA
Migrating from StuffDocumentsChain
Release policy
Security
Tutorials
Build a Simple LLM Application with LCEL
On this page
Build a Simple LLM Application with LCEL
In this quickstart we'll show you how to build a simple LLM application with LangChain. This application will translate text from English into another language. This is a relatively simple LLM application - it's just a single LLM call plus some prompting. Still, this is a great way to get started with LangChain - a lot of features can be built with just some prompting and an LLM call!
After reading this tutorial, you'll have a high level overview of:
Using 
language models
Using 
PromptTemplates
 and 
OutputParsers
Using 
LangChain Expression Language (LCEL)
 to chain components together
Debugging and tracing your application using 
LangSmith
Deploying your application with 
LangServe
Let's dive in!
Setup
‚Äã
Jupyter Notebook
‚Äã
This guide (and most of the other guides in the documentation) uses 
Jupyter notebooks
 and assumes the reader is as well. Jupyter notebooks are perfect for learning how to work with LLM systems because oftentimes things can go wrong (unexpected output, API down, etc) and going through guides in an interactive environment is a great way to better understand them.
This and other tutorials are perhaps most conveniently run in a Jupyter notebook. See 
here
 for instructions on how to install.
Installation
‚Äã
To install LangChain run:
Pip
Conda
pip 
install
 langchain
conda 
install
 langchain -c conda-forge
For more details, see our 
Installation guide
.
LangSmith
‚Äã
Many of the applications you build with LangChain will contain multiple steps with multiple invocations of LLM calls.
As these applications get more and more complex, it becomes crucial to be able to inspect what exactly is going on inside your chain or agent.
The best way to do this is with 
LangSmith
.
After you sign up at the link above, make sure to set your environment variables to start logging traces:
export
 
LANGCHAIN_TRACING_V2
=
"true"
export
 
LANGCHAIN_API_KEY
=
"..."
Or, if in a notebook, you can set them with:
import
 getpass
import
 os
os
.
environ
[
"LANGCHAIN_TRACING_V2"
]
 
=
 
"true"
os
.
environ
[
"LANGCHAIN_API_KEY"
]
 
=
 getpass
.
getpass
(
)
Using Language Models
‚Äã
First up, let's learn how to use a language model by itself. LangChain supports many different language models that you can use interchangeably - select the one you want to use below!
OpenAI
Anthropic
Azure
Google
Cohere
NVIDIA
FireworksAI
Groq
MistralAI
TogetherAI
pip 
install
 -qU langchain-openai
import
 getpass
import
 os
os
.
environ
[
"OPENAI_API_KEY"
]
 
=
 getpass
.
getpass
(
)
from
 langchain_openai 
import
 ChatOpenAI
model 
=
 ChatOpenAI
(
model
=
"gpt-4"
)
pip 
install
 -qU langchain-anthropic
import
 getpass
import
 os
os
.
environ
[
"ANTHROPIC_API_KEY"
]
 
=
 getpass
.
getpass
(
)
from
 langchain_anthropic 
import
 ChatAnthropic
model 
=
 ChatAnthropic
(
model
=
"claude-3-5-sonnet-20240620"
)
pip 
install
 -qU langchain-openai
import
 getpass
import
 os
os
.
environ
[
"AZURE_OPENAI_API_KEY"
]
 
=
 getpass
.
getpass
(
)
from
 langchain_openai 
import
 AzureChatOpenAI
model 
=
 AzureChatOpenAI
(
    azure_endpoint
=
os
.
environ
[
"AZURE_OPENAI_ENDPOINT"
]
,
    azure_deployment
=
os
.
environ
[
"AZURE_OPENAI_DEPLOYMENT_NAME"
]
,
    openai_api_version
=
os
.
environ
[
"AZURE_OPENAI_API_VERSION"
]
,
)
pip 
install
 -qU langchain-google-vertexai
import
 getpass
import
 os
os
.
environ
[
"GOOGLE_API_KEY"
]
 
=
 getpass
.
getpass
(
)
from
 langchain_google_vertexai 
import
 ChatVertexAI
model 
=
 ChatVertexAI
(
model
=
"gemini-1.5-flash"
)
pip 
install
 -qU langchain-cohere
import
 getpass
import
 os
os
.
environ
[
"COHERE_API_KEY"
]
 
=
 getpass
.
getpass
(
)
from
 langchain_cohere 
import
 ChatCohere
model 
=
 ChatCohere
(
model
=
"command-r-plus"
)
pip 
install
 -qU langchain-nvidia-ai-endpoints
import
 getpass
import
 os
os
.
environ
[
"NVIDIA_API_KEY"
]
 
=
 getpass
.
getpass
(
)
from
 langchain 
import
 ChatNVIDIA
model 
=
 ChatNVIDIA
(
model
=
"meta/llama3-70b-instruct"
)
pip 
install
 -qU langchain-fireworks
import
 getpass
import
 os
os
.
environ
[
"FIREWORKS_API_KEY"
]
 
=
 getpass
.
getpass
(
)
from
 langchain_fireworks 
import
 ChatFireworks
model 
=
 ChatFireworks
(
model
=
"accounts/fireworks/models/llama-v3p1-70b-instruct"
)
pip 
install
 -qU langchain-groq
import
 getpass
import
 os
os
.
environ
[
"GROQ_API_KEY"
]
 
=
 getpass
.
getpass
(
)
from
 langchain_groq 
import
 ChatGroq
model 
=
 ChatGroq
(
model
=
"llama3-8b-8192"
)
pip 
install
 -qU langchain-mistralai
import
 getpass
import
 os
os
.
environ
[
"MISTRAL_API_KEY"
]
 
=
 getpass
.
getpass
(
)
from
 langchain_mistralai 
import
 ChatMistralAI
model 
=
 ChatMistralAI
(
model
=
"mistral-large-latest"
)
pip 
install
 -qU langchain-openai
import
 getpass
import
 os
os
.
environ
[
"TOGETHER_API_KEY"
]
 
=
 getpass
.
getpass
(
)
from
 langchain_openai 
import
 ChatOpenAI
model 
=
 ChatOpenAI
(
    base_url
=
"https://api.together.xyz/v1"
,
    api_key
=
os
.
environ
[
"TOGETHER_API_KEY"
]
,
    model
=
"mistralai/Mixtral-8x7B-Instruct-v0.1"
,
)
Let's first use the model directly. 
ChatModel
s are instances of LangChain "Runnables", which means they expose a standard interface for interacting with them. To just simply call the model, we can pass in a list of messages to the 
.invoke
 method.
from
 langchain_core
.
messages 
import
 HumanMessage
,
 SystemMessage
messages 
=
 
[
    SystemMessage
(
content
=
"Translate the following from English into Italian"
)
,
    HumanMessage
(
content
=
"hi!"
)
,
]
model
.
invoke
(
messages
)
API Reference:
HumanMessage
 | 
SystemMessage
AIMessage(content='ciao!', response_metadata={'token_usage': {'completion_tokens': 3, 'prompt_tokens': 20, 'total_tokens': 23}, 'model_name': 'gpt-4', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-fc5d7c88-9615-48ab-a3c7-425232b562c5-0')
If we've enabled LangSmith, we can see that this run is logged to LangSmith, and can see the 
LangSmith trace
OutputParsers
‚Äã
Notice that the response from the model is an 
AIMessage
. This contains a string response along with other metadata about the response. Oftentimes we may just want to work with the string response. We can parse out just this response by using a simple output parser.
We first import the simple output parser.
from
 langchain_core
.
output_parsers 
import
 StrOutputParser
parser 
=
 StrOutputParser
(
)
API Reference:
StrOutputParser
One way to use it is to use it by itself. For example, we could save the result of the language model call and then pass it to the parser.
result 
=
 model
.
invoke
(
messages
)
parser
.
invoke
(
result
)
'Ciao!'
More commonly, we can "chain" the model with this output parser. This means this output parser will get called every time in this chain. This chain takes on the input type of the language model (string or list of message) and returns the output type of the output parser (string).
We can easily create the chain using the 
|
 operator. The 
|
 operator is used in LangChain to combine two elements together.
chain 
=
 model 
|
 parser
chain
.
invoke
(
messages
)
'Ciao!'
If we now look at LangSmith, we can see that the chain has two steps: first the language model is called, then the result of that is passed to the output parser. We can see the 
LangSmith trace
Prompt Templates
‚Äã
Right now we are passing a list of messages directly into the language model. Where does this list of messages come from? Usually, it is constructed from a combination of user input and application logic. This application logic usually takes the raw user input and transforms it into a list of messages ready to pass to the language model. Common transformations include adding a system message or formatting a template with the user input.
PromptTemplates are a concept in LangChain designed to assist with this transformation. They take in raw user input and return data (a prompt) that is ready to pass into a language model. 
Let's create a PromptTemplate here. It will take in two user variables:
language
: The language to translate text into
text
: The text to translate
from
 langchain_core
.
prompts 
import
 ChatPromptTemplate
API Reference:
ChatPromptTemplate
First, let's create a string that we will format to be the system message:
system_template 
=
 
"Translate the following into {language}:"
Next, we can create the PromptTemplate. This will be a combination of the 
system_template
 as well as a simpler template for where to put the text to be translated
prompt_template 
=
 ChatPromptTemplate
.
from_messages
(
    
[
(
"system"
,
 system_template
)
,
 
(
"user"
,
 
"{text}"
)
]
)
The input to this prompt template is a dictionary. We can play around with this prompt template by itself to see what it does by itself
result 
=
 prompt_template
.
invoke
(
{
"language"
:
 
"italian"
,
 
"text"
:
 
"hi"
}
)
result
ChatPromptValue(messages=[SystemMessage(content='Translate the following into italian:'), HumanMessage(content='hi')])
We can see that it returns a 
ChatPromptValue
 that consists of two messages. If we want to access the messages directly we do:
result
.
to_messages
(
)
[SystemMessage(content='Translate the following into italian:'),
 HumanMessage(content='hi')]
Chaining together components with LCEL
‚Äã
We can now combine this with the model and the output parser from above using the pipe (
|
) operator:
chain 
=
 prompt_template 
|
 model 
|
 parser
chain
.
invoke
(
{
"language"
:
 
"italian"
,
 
"text"
:
 
"hi"
}
)
'ciao'
This is a simple example of using 
LangChain Expression Language (LCEL)
 to chain together LangChain modules. There are several benefits to this approach, including optimized streaming and tracing support.
If we take a look at the LangSmith trace, we can see all three components show up in the 
LangSmith trace
.
Serving with LangServe
‚Äã
Now that we've built an application, we need to serve it. That's where LangServe comes in.
LangServe helps developers deploy LangChain chains as a REST API. You do not need to use LangServe to use LangChain, but in this guide we'll show how you can deploy your app with LangServe.
While the first part of this guide was intended to be run in a Jupyter Notebook or script, we will now move out of that. We will be creating a Python file and then interacting with it from the command line.
Install with:
pip 
install
 
"langserve[all]"
Server
‚Äã
To create a server for our application we'll make a 
serve.py
 file. This will contain our logic for serving our application. It consists of three things:
The definition of our chain that we just built above
Our FastAPI app
A definition of a route from which to serve the chain, which is done with 
langserve.add_routes
#!/usr/bin/env python
from
 fastapi 
import
 FastAPI
from
 langchain_core
.
prompts 
import
 ChatPromptTemplate
from
 langchain_core
.
output_parsers 
import
 StrOutputParser
from
 langchain_openai 
import
 ChatOpenAI
from
 langserve 
import
 add_routes
# 1. Create prompt template
system_template 
=
 
"Translate the following into {language}:"
prompt_template 
=
 ChatPromptTemplate
.
from_messages
(
[
    
(
'system'
,
 system_template
)
,
    
(
'user'
,
 
'{text}'
)
]
)
# 2. Create model
model 
=
 ChatOpenAI
(
)
# 3. Create parser
parser 
=
 StrOutputParser
(
)
# 4. Create chain
chain 
=
 prompt_template 
|
 model 
|
 parser
# 4. App definition
app 
=
 FastAPI
(
  title
=
"LangChain Server"
,
  version
=
"1.0"
,
  description
=
"A simple API server using LangChain's Runnable interfaces"
,
)
# 5. Adding chain route
add_routes
(
    app
,
    chain
,
    path
=
"/chain"
,
)
if
 __name__ 
==
 
"__main__"
:
    
import
 uvicorn
    uvicorn
.
run
(
app
,
 host
=
"localhost"
,
 port
=
8000
)
API Reference:
ChatPromptTemplate
 | 
StrOutputParser
 | 
ChatOpenAI
And that's it! If we execute this file:
python serve.py
we should see our chain being served at 
http://localhost:8000
.
Playground
‚Äã
Every LangServe service comes with a simple 
built-in UI
 for configuring and invoking the application with streaming output and visibility into intermediate steps.
Head to 
http://localhost:8000/chain/playground/
 to try it out! Pass in the same inputs as before - 
{"language": "italian", "text": "hi"}
 - and it should respond same as before.
Client
‚Äã
Now let's set up a client for programmatically interacting with our service. We can easily do this with the 
langserve.RemoteRunnable
.
Using this, we can interact with the served chain as if it were running client-side.
from
 langserve 
import
 RemoteRunnable
remote_chain 
=
 RemoteRunnable
(
"http://localhost:8000/chain/"
)
remote_chain
.
invoke
(
{
"language"
:
 
"italian"
,
 
"text"
:
 
"hi"
}
)
'Ciao'
To learn more about the many other features of LangServe 
head here
.
Conclusion
‚Äã
That's it! In this tutorial you've learned how to create your first simple LLM application. You've learned how to work with language models, how to parse their outputs, how to create a prompt template, chaining them with LCEL, how to get great observability into chains you create with LangSmith, and how to deploy them with LangServe.
This just scratches the surface of what you will want to learn to become a proficient AI Engineer. Luckily - we've got a lot of other resources!
For further reading on the core concepts of LangChain, we've got detailed 
Conceptual Guides
.
If you have more specific questions on these concepts, check out the following sections of the how-to guides:
LangChain Expression Language (LCEL)
Prompt templates
Chat models
Output parsers
LangServe
And the LangSmith docs:
LangSmith
Edit this page
Was this page helpful?
You can also leave detailed feedback
 
on GitHub
.
Previous
Tutorials
Next
Build a Query Analysis System
Setup
Jupyter Notebook
Installation
LangSmith
Using Language Models
OutputParsers
Prompt Templates
Chaining together components with LCEL
Serving with LangServe
Server
Playground
Client
Conclusion
Community
Twitter
GitHub
Organization
Python
JS/TS
More
Homepage
Blog
YouTube
Copyright ¬© 2024 LangChain, Inc.







